{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from numpy import asarray\n",
                "from numpy import expand_dims\n",
                "import numpy as np\n",
                "import pickle\n",
                "import cv2\n",
                "from architecture import * \n",
                "from mtcnn import MTCNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-07-12 11:53:46.470258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
                    ]
                }
            ],
            "source": [
                "#Create detector model\n",
                "detector = MTCNN()\n",
                "\n",
                "# Create the FaceNet model\n",
                "face_encoder = InceptionResNetV2()\n",
                "face_encoder.load_weights('facenet_keras.h5')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"data.pkl\", \"rb\") as myfile:\n",
                "    database = pickle.load(myfile) \n",
                "    myfile.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "OpenCV: camera access has been denied. Either run 'tccutil reset Camera' command in same terminal to reset application authorization status, either modify 'System Preferences -> Security & Privacy -> Camera' settings for your application.\n",
                        "OpenCV: camera failed to properly initialize!\n"
                    ]
                }
            ],
            "source": [
                "cap = cv2.VideoCapture(0)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "OpenCV: camera access has been denied. Either run 'tccutil reset Camera' command in same terminal to reset application authorization status, either modify 'System Preferences -> Security & Privacy -> Camera' settings for your application.\n",
                        "OpenCV: camera failed to properly initialize!\n"
                    ]
                },
                {
                    "ename": "InvalidImage",
                    "evalue": "Image not valid.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mInvalidImage\u001b[0m                              Traceback (most recent call last)",
                        "\u001b[1;32m/Users/cole.harrison/Desktop/jcoleharrison/facial_recognition/src/v2/recognition_nb.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cole.harrison/Desktop/jcoleharrison/facial_recognition/src/v2/recognition_nb.ipynb#ch0000005?line=2'>3</a>\u001b[0m \u001b[39mwhile\u001b[39;00m(\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cole.harrison/Desktop/jcoleharrison/facial_recognition/src/v2/recognition_nb.ipynb#ch0000005?line=3'>4</a>\u001b[0m     _, image \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cole.harrison/Desktop/jcoleharrison/facial_recognition/src/v2/recognition_nb.ipynb#ch0000005?line=5'>6</a>\u001b[0m     faces \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39;49mdetect_faces(frame)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cole.harrison/Desktop/jcoleharrison/facial_recognition/src/v2/recognition_nb.ipynb#ch0000005?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(faces)\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cole.harrison/Desktop/jcoleharrison/facial_recognition/src/v2/recognition_nb.ipynb#ch0000005?line=8'>9</a>\u001b[0m         x1, y1, width, height \u001b[39m=\u001b[39m faces[\u001b[39m0\u001b[39m]        \n",
                        "File \u001b[0;32m~/Desktop/jcoleharrison/facial_recognition/env/facial_recognition/lib/python3.8/site-packages/mtcnn/mtcnn.py:285\u001b[0m, in \u001b[0;36mMTCNN.detect_faces\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39mDetects bounding boxes from the specified image.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39m:param img: image to process\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m:return: list containing all the bounding boxes detected with their keypoints.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m img \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidImage(\u001b[39m\"\u001b[39m\u001b[39mImage not valid.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m height, width, _ \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mshape\n\u001b[1;32m    288\u001b[0m stage_status \u001b[39m=\u001b[39m StageStatus(width\u001b[39m=\u001b[39mwidth, height\u001b[39m=\u001b[39mheight)\n",
                        "\u001b[0;31mInvalidImage\u001b[0m: Image not valid."
                    ]
                }
            ],
            "source": [
                "cap = cv2.VideoCapture(0)\n",
                "while cap.isOpened(): \n",
                "    _, image = cap.read()\n",
                "    \n",
                "    faces = detector.detect_faces(image)\n",
                "    \n",
                "    if len(faces)>0:\n",
                "        x1, y1, width, height = faces[0]['box']\n",
                "    else:\n",
                "        x1, y1, width, height = 1, 1, 10, 10\n",
                "    \n",
                "    x1, y1 = abs(x1), abs(y1)\n",
                "    x2, y2 = x1 + width, y1 + height\n",
                "    \n",
                "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "    face = image[y1:y2, x1:x2]                        \n",
                "    face = cv2.resize(face, (160, 160))\n",
                "    face = asarray(face)\n",
                "    \n",
                "    face = face.astype('float32')\n",
                "    mean, std = face.mean(), face.std()\n",
                "    face = (face - mean) / std\n",
                "    \n",
                "    face = expand_dims(face, axis=0)\n",
                "    signature = face_encoder.predict(face)\n",
                "    \n",
                "    min_dist=100\n",
                "    identity=' '\n",
                "    for key, value in database.items():\n",
                "        dist = np.linalg.norm(value-signature)\n",
                "        if dist < min_dist:\n",
                "            min_dist = dist\n",
                "            identity = key\n",
                "            \n",
                "    cv2.putText(image,identity, (100,100),cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
                "    cv2.rectangle(image,(x1,y1),(x2,y2), (0,255,0), 2)\n",
                "        \n",
                "    cv2.imshow('res',image)\n",
                "    \n",
                "    k = cv2.waitKey(5) & 0xFF\n",
                "    if k == 27:\n",
                "        break\n",
                "        \n",
                "cv2.destroyAllWindows()\n",
                "cap.release()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.9 ('facial_recognition': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.9"
        },
        "vscode": {
            "interpreter": {
                "hash": "0e96f4b903cfe7bfa552078bedac6c84a8e21021f8101ee44ebe7a6b4e21e02f"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
